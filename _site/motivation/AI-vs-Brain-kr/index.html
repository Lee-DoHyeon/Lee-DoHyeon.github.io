<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="us" class="no-js">
  <head>
    <meta charset="utf-8" />

<!-- begin _includes/seo.html --><title>인공지능 vs 두뇌  |  Do-Hyeon Lee</title>
<meta name="description" content="">


  <meta name="author" content="Do-Hyeon Lee">
  
  <meta property="article:author" content="Do-Hyeon Lee">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="us_EN">
<meta property="og:site_name" content="Do-Hyeon Lee">
<meta property="og:title" content="인공지능 vs 두뇌">
<meta property="og:url" content="http://localhost:4000/motivation/AI-vs-Brain-kr/">


  <meta property="og:description" content="">



  <meta property="og:image" content="http://localhost:4000/assets/images/posts/2023/Q2/2023-05-24-AI%20vs%20Brain/0.jpeg">





  <meta property="article:published_time" content="2023-05-24T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/motivation/AI-vs-Brain-kr/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Do-Hyeon Lee",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->
 
<link
  href="/feed.xml"
  type="application/atom+xml"
  rel="alternate"
  title="Do-Hyeon Lee Feed"
/>


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />

<script>
  document.documentElement.className =
    document.documentElement.className.replace(/\bno-js\b/g, "") + " js ";
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css" />
<link
  rel="preload"
  href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"
  as="style"
  onload="this.onload=null;this.rel='stylesheet'"
/>
<noscript
  ><link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"
/></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link
  rel="apple-touch-icon"
  sizes="152x152"
  href="/favicon.io/apple-touch-icon.png"
/>
<link
  rel="icon"
  type="image/png"
  sizes="32x32"
  href="/favicon.io/favicon-32x32.png"
/>
<link
  rel="icon"
  type="image/png"
  sizes="16x16"
  href="/favicon.io/favicon-16x16.png"
/>
<link rel="manifest" href="/favicon.io/site.webmanifest" />
<meta name="msapplication-TileColor" content="#da532c" />
<meta name="theme-color" content="#ffffff" />
<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Do-Hyeon Lee
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/research/">Research</a>
            </li><li class="masthead__menu-item">
              <a href="/watch/">Watch</a>
            </li><li class="masthead__menu-item">
              <a href="/read/">Read</a>
            </li><li class="masthead__menu-item">
              <a href="/outreach/">Outreach</a>
            </li><li class="masthead__menu-item">
              <a href="/cv/">CV</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/avatar.jpg" alt="Do-Hyeon Lee" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Do-Hyeon Lee</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Human and Machine Mind-Behavior-Brain Researcher</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://maps.app.goo.gl/fDF8TMFHnu1fHCBd9" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i><span class="label">Location</span></a></li>
          
        
          
            <li><a href="mailto:lead.o.hyeon@gmail.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://x.com/HumMachCoevol" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/dohyeon-lee-4793a6244" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">Linkedin</span></a></li>
          
        
          
            <li><a href="https://www.youtube.com/@leadh99" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i><span class="label">YouTube</span></a></li>
          
        
          
            <li><a href="https://github.com/Lee-DoHyeon" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="인공지능 vs 두뇌">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="2023-05-24T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/motivation/AI-vs-Brain-kr/" class="u-url" itemprop="url">인공지능 vs 두뇌
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> 인공지능 vs 두뇌</h4></header>
              <ul class="toc__menu"><li><a href="#1-무한한-세상을-유한한-공간에-담다">1. 무한한 세상을 유한한 공간에 담다</a></li><li><a href="#2-현재의-성공이-미래의-실패가-되다">2. 현재의 성공이 미래의 실패가 되다</a></li><li><a href="#3-민감한-만큼-둔감해지니-전체가-보인다">3. 민감한 만큼 둔감해지니 전체가 보인다</a></li><li><a href="#4-지극히-주관적이다-그래서-더-객관적이다">4. 지극히 주관적이다, 그래서 더 객관적이다</a></li><li><a href="#5-과거를-예측하고-미래를-회상하다">5. 과거를 예측하고 미래를 회상하다</a></li><li><a href="#6-생각이-시간과-공간을-넘나드는-마법을-부리다">6. 생각이 시간과 공간을 넘나드는 마법을 부리다</a></li><li><a href="#7-미래를-내다보며-과거를-바꾼다">7. 미래를 내다보며 과거를 바꾼다</a><ul><li><a href="#reference">Reference</a></li></ul></li></ul></li></ul>

            </nav>
          </aside>
        
        <style>
  .centered-container {
      text-align: center;
  }
  figure {
      display: inline-block;
      margin: auto;
      padding: 10px;
      text-align: center;
      background-color: #fff;
  }
  figcaption {
      font-family: "Wanted Sans Variable", "Wanted Sans";
      font-size: 12px;
      color: #555;
      margin-top: 5px;
  }
</style>

<blockquote>
  <p>“우리에게 너무나 어려운 문제들, 인공지능은 어떻게 풀어내는가?
우리에게는 너무나 쉬운 문제들, 인공지능은 왜 못 풀어내는가?
인공지능의 관점에서 바라보는 인간지능이란 무엇인가?”</p>
</blockquote>

<blockquote>
  <p>본 글은 이상완 교수님께서 저술하신 “인공지능과 뇌는 어떻게 생각하는가[1]”를 읽고 정리한 글입니다.</p>
</blockquote>

<h1 id="1-무한한-세상을-유한한-공간에-담다">1. 무한한 세상을 유한한 공간에 담다</h1>

<p>해당 챕터는 사과를 어떻게 인식Recognition하는지, <strong>연관짓기 문제Binding Problem</strong>를 중심으로 추상화와 다양성의 딜레마를 소개한다. 이를 해결하기 위한 초기 인공지능 연구의 역사적 흐름을 소개한다. Marvin Minsky의 Feedforward Network에서 시작한 추상화 과정과, 여기서 발생하는 Credit Assignment Problem을 해결하기 위한 Error Backpropagation 알고리즘을 소개한다.</p>

<hr />

<h1 id="2-현재의-성공이-미래의-실패가-되다">2. 현재의 성공이 미래의 실패가 되다</h1>

<p>첫번째 챕터에서는 단순히 주어진 현재의 문제에 집중했다면, 두번째 챕터는 현실 상황에서 상호작용하는 통계적 상황에 집중한다. 통계적 기계학습의 관점에서 Structural Risk Minimization을 소개하며, Underfitting/Overfitting을 막기 위한 Tikhonov Regularization 방법을 소개한다. 더 나아가, Dual Problem을 생각하여 Support Vector Machine(SVM)을 소개한다.</p>

<p>Bias-Variance Tradeoff를 해결하기 위해 채택한 “모델의 효율성”에 대한 추구는 실제 뇌에서도 발견된다고 한다. 실제 배우 제니퍼 애니스톤을 인식하는 뉴런이 있다고 한다[2]! 이는 하나의 개념을 안정적으로 인지하는 데에 필요한 활성 세포의 개수가 아주 적다는 <strong>희소 표상sparse representation</strong>을 의미한다고 한다. 희소 표상과 관련해 발달인지 분야의 결정적 시기Critical Period와 가지치기Pruning도 관련이 깊다. 이러한 에너지 효율성은 인공지능 시스템이 돌아가는 워크스테이션에 비해 적어도 400배 이상 효율적이라고 간주할 수 있다고 한다.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/1.jpeg" style="width: 80%; height: auto;" />
    <figcaption>
      뇌와 인공지능 시스템의 가장 큰 차이점: 에너지 효율성!
    </figcaption>
  </figure>
</div>

<hr />

<h1 id="3-민감한-만큼-둔감해지니-전체가-보인다">3. 민감한 만큼 둔감해지니 전체가 보인다</h1>

<p>첫번째 챕터에서 기본적인 추상화/다양성의 문제를 다루었고, 두번째 챕터에서 통계적인 해법과 우리 뇌의 특성을 확인했다. 세번째 챕터에서는 궁극적으로 해결하고자 하는 Specificity-Invariance Dilemma를 다룬다. 민감하면서도 둔감하도록 개념을 추상화하기 위한 해결책으로 (1)필터링, (2)풀링, (3)국소성이 있다.</p>

<p>먼저 (1)필터링의 경우, 컨볼루션 연산을 통해 특정 패턴을 나타내는 정보를 필터링하여 중요한 정보만 확인하고, 나머지는 무시하게 할 수 있다. 다음으로, (2)풀링 혹은 서브샘플링을 활용해 유의미한 정보만을 압축하여 추출하게 할 수 있다. 마지막으로, (3)앞선 필터링과 풀링 모두에 적용되는 국소성으로 주위 정보 수용 영역Receptive Field의 범위를 직접 설정하여 유의미한 국소적 정보의 범위를 채택할 수 있다. 이러한 <strong>국소적 배열 패턴Topographic Map</strong>은 실제 뇌에서도 등장한다. 일종의 사전 지식을 구조적으로 함축하는 것으로, 의도적으로 귀납적 편향성Inductive Bias을 주는 것이다. 이를 통해 <u>계산의 시공간적, 에너지적 효율성</u>을 높일 수 있다. 이것의 기원에 대한 이론은 아직 드러나지 않았지만, 진화를 통해 획득한 형질이 아닐까 싶다.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/2.jpeg" style="width: 80%; height: auto;" />
    <figcaption>
      대뇌 피질에서 확인할 수 있는 운동 정보 사이의 국소적 배열 패턴. 과연 이러한 국소적인 배열 패턴이 계산의 효율성을 높여줄 것인가?
    </figcaption>
  </figure>
</div>

<p>이제 컨볼루션 및 풀링 연산, 비선형 함수을 묶어 빌딩 블록Building Block을 만든 후, 다양한 방식으로 쌓아 아키텍처를 고안한다. 이를 통해 정말 원하는 대로 추상화할 수 있도록 해결한다. 이처럼 상향식으로 부분을 통해 전체를 보았다면, 반대로, 하향식으로 전체를 통해 부분을 확인할 수 있는 Attention이 가능하다. 이를 위해 Self-Attention이라는 기술이 등장했지만, 아직 깔끔한 해결책이 등장하진 않았다고 한다.</p>

<p>우리의 뇌는 이러한 부분과 전체의 모순을 해결하기 위해 상향식 개념 추상화와 하향식 주의집중을 하게 된다. 이를 가능하게 하는 수많은 메커니즘 중, 가장 낮은 층에서 상위 개념을 잡아낼 수 있는 메커니즘이 있다고 한다. 이는 망막 신경이 중심에서 주변으로 갈 수록 밀도가 낮아지는 비선형성을 활용한 것으로, 밀도가 낮아 저차원인 주변부의 정보를 최상부로 귀띰해주어 주의집중을 야기하는 원리이다.</p>

<ul>
  <li>Brain -&gt; AI:
    <ul>
      <li>대뇌피질의 계산적 원리가 반영된 CNN이 큰 성공을 거두기까지 20년이 걸림.</li>
      <li>뇌과학이 도움을 주었으면 더 일찍 가능했을 것.</li>
    </ul>
  </li>
  <li>AI -&gt; Brain:
    <ul>
      <li>CNN과 두뇌의 기능적 유사성에 주목, 대뇌의 정보처리 과정을 연구해보니 각 층의 개별 뉴런과 유사하게 작동했음. 상부상조.</li>
      <li>Predictive Coding Scheme, Illusion?</li>
    </ul>
  </li>
</ul>

<p>| Questions:</p>
<ol>
  <li>인공신경망과 두뇌의 기능적Functional 유사성이 있다면, 정보 처리의 관점 뿐만 아니라 내적 경험Introspection의 관점에서도 유사한 현상을 관찰할 수 있을 것이다. 인간의 착시/환각 현상과 인공신경망에서의 현상은 무슨 관계가 있을까?</li>
  <li>대뇌피질에 표상이 국소적으로 배열된 것을 통해 계산의 시공간적, 에너지적 효율성을 높인다는 것이 과연 유의미한 주장일까? 계산모형과 시뮬레이션을 통해 검증해볼 수 있지 않을까?</li>
</ol>

<hr />

<h1 id="4-지극히-주관적이다-그래서-더-객관적이다">4. 지극히 주관적이다, 그래서 더 객관적이다</h1>

<p>챕터 1~3까지는 어떻게 세상World에서 관측한 결과를 바탕으로 추상화하여 이해할 수 있는가?라는 질문이었다. 챕터 4는 추상화의 반대인 구체화에 대한 이야기로 데이터의 생성을 주제로 한다. 결정론적이었던 앞선 신경망과 달리, 이제부터는 각 뉴론을 확률변수로 대체한 <strong>확률론적 신경망</strong>을 다루게 된다.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/3.png" style="width: 80%; height: auto;" />
    <figcaption>
      Hopfield Network, Boltzmann Machine, Restricted BM, The Neural Network Zoo[3]
    </figcaption>
  </figure>
</div>

<p>초기 확률론적 신경망으로 <strong><u>홉필드 네트워크Hopfield Network; HN</u></strong>가 있으며, 단순한 통계분석 뿐만 아니라 복잡한 상관관계 분석이 가능하다. 확률론적 신경망은 (1)쉽게 추론이 가능하고, (2)각 요소의 역할을 말해줄 수 있으며, (3)한 요소를 무시하면 잘못된 결론에 도출될 수 있음을 알려줄 뿐만 아니라, (4)꽤 복잡한 상호관계를 찾아낼 수 있다는 장점이 있다.</p>

<p>기존의 홉필드 네트워크에 은닉 뉴런을 추가한 <strong><u>볼츠만 머신Boltzmann Machine; BM</u></strong>은 데이터로부터 가설을 검증할 수 있게 해준다. 이 모형의 목적은 데이터로부터 다양한 인자들 간의 상관관계를 배우는 것이다. 뉴런들 사이를 연결하는 최적의 가중치를 찾아나가는 학습 규칙을 유도하여, (1)데이터 안에서 직접 관찰할 수 있는 인자들 간의 상관성과 (2)네트워크가 현재까지 학습한 상관성 간의 차이를 줄이는 방향으로 이루어진다고 한다. (1)의 경우 데이터를 직접 보면서 계산되는 부분이므로 깨어있는 상태, (2)의 경우 데이터를 보지 않고 네트워크가 추론하는 부분이므로 꿈꾸는 상태로 비유할 수 있어, Wake-Sleep Rule이라고도 불린다.</p>

<p>확률적 신경망이 실제 적용되기 어렵다는 문제를 해결하고자 <strong><u>제한적 볼츠만 머신Restricted Boltzmann Machine; RBM</u></strong>이 등장하게 된다. RBM은 일반 뉴런Visible Neuron과 은닉 뉴런Hidden Neuron 사이의 연결만을 허용하는 방식을 취하여, 은닉 뉴런 사이의 복잡한 상관관계를 사라지게 만든다. 이러한 일반 뉴런층과 은닉 뉴런층의 구조는 하나의 빌딩 블록을 형성하여, 심층 제한적 볼츠만 머신Deep RBM; DRBM이 등장하게 된다. 이때, 오차 역전파 알고리즘을 활용하여 정밀한 학습을 수행하면 결정론적 신경망인 오토인코더Autoencoder;AE가 되게 된다.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/4.png" style="width: 80%; height: auto;" />
    <figcaption>
      Autoencoder &amp; Variational Autoencoder, The Neural Network Zoo[3]
    </figcaption>
  </figure>
</div>

<p>오토인코더와 같이 대칭성을 활용해 개념의 추상화와 구체화를 구현한 것은 주성분분석PCA에서도 확인할 수 있다. 현대에는 대칭성이 깨지더라도 수학적으로 허용되는 선에서 활용되고 있다. 오토인코더의 확률론적 버전으로 <strong><u>변분 오토인코더Variational Autoencoder; VAE</u></strong>가 있으며, (1)오토인코더의 대칭성을 어느정도 유지하면서, (2)추상화 과정에 깊이를 더하고, (3)구체화 과정의 밀도를 높이는 장점을 갖는다고 한다. 마지막으로 <strong><u>재매개변수화Reparametrization</u></strong> 트릭이 적용되어 오차 역전파 알고리즘도 적용될 수 있게 된다.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/5.png" style="width: 80%; height: auto;" />
    <figcaption>
      GAN, The Neural Network Zoo[3]
    </figcaption>
  </figure>
</div>

<p>한편, 추상화와 구체화의 성능은 결국 사용자가 정의한 오차함수에 의해 평가되는데, 이러한 자기 평가를 객관화할 수 있도록 하게 위해 게임 이론을 활용하여 <strong><u>생성적 적대적 신경망Generative Adversarial Network; GAN</u></strong>같은 모형이 등장했다. 정보 이론의 관점에서, 기존의 주성분 분석이나 오토 인코더와 같은 결정적 신경망은 KL Divergence를 최소화하는 방식으로 학습된다. 비대칭적인 KL Divergence와 달리, GAN은 대칭적인 JS Divergence를 최소화하게 된다. 이는 경제학의 <strong><u>Optimal Transport</u></strong> 문제로 간주할 수 있고 Wasserstein Distance로 평가할 수 있다.</p>

<p>| Questions:</p>
<ol>
  <li>각 확률론적 신경망은 인간의 두뇌와 어떤 관련이 있는가? 뉴런이 확률변수라고 가정하는 것은 실제 뉴런의 랜덤성 때문인가, 아니면 단순한 우리의 가정일 뿐인가?</li>
  <li>최근 등장하는 StableDiffusion과 같은 Diffusion 기반 생성 모형들은 앞서 설명한 모형들과 어떤 관련성이 있는가?</li>
  <li>통계역학의 Ising Model, Cellular Automata와 어떤 관련성이 있는가?</li>
</ol>

<hr />

<h1 id="5-과거를-예측하고-미래를-회상하다">5. 과거를 예측하고 미래를 회상하다</h1>

<p>챕터 1~3까지 개념의 추상화, 챕터 4에서는 개념의 구체화에 대해 다루었다. 이제는 개념의 시간 여행; 기억에 대해 다룬다. 이전까지는 데이터의 <u>i.i.d</u> 조건을 가정하게 된다. 그러나, 현실은 대부분 독립 사건이 아니다. 때문에, 과거에 생성한 개념을 잊지 않고 기억하는 것이 중요해진다. 이를 구현하기 위한 전략으로 <strong>되먹임Feedback</strong>이 있다. 실제 대뇌피질, 해마 등에서 가장 많이 발견되는 뉴론 중 하나인 <strong><u>피라미드 뉴런Pyramidal Neuron</u></strong>은 근처 뉴런으로부터 입력을 받을 뿐만 아니라, 이미 정보를 전달한 꽤 멀리 있는 뉴런으로부터 입력을 거꾸로 받기도 한다! <strong><u>되먹임 신경망Recurrent Neural Network; RNN</u></strong>에서 오차 역전파 알고리즘을 적용하기엔 <u>역전파 발산Exploding Gradient</u> 혹은 <u>역전파 소멸Vanishing Gradient</u>같은 현상이 발생하여 학습이 잘 되지 않는다. 이러한 문제를 해결하고자 장기 기억과 단기 기억을 결합한 <strong><u>장단기 메모리Long-Short Term Memory; LSTM</u></strong>이 있다.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/6.png" style="width: 80%; height: auto;" />
    <figcaption>
      RNN &amp; LSTM &amp; GRU;The Neural Network Zoo[3]
    </figcaption>
  </figure>
</div>

<p>새로운 정보를 유연하게 받아들이면서 오랫동안 안정적으로 기억을 유지할 수 있는 이상적인 신경망은 (1)상황에 적응할 수 있을 뿐만 아니라, (2)새로운 지식을 확장할 수도 있다. 이러한 의미에서 <strong>연속 학습Continual Learning</strong> 혹은 <strong>평생 학습Lifelong Learning</strong>이라고 불린다. 더욱이 다양한 작업이나 목표에도 적응할 수 있다는 점에서 다중 도메인 적응Multi-Domain Adaptation, 혹은 다중 작업 학습Multi-Task Learning에도 사용될 수 있다.</p>

<p>인공 신경망은 유동적인 기억 능력이 부족하여, 새로운 정보를 쫓느라 과거의 학습 정보가 증발되는 <strong><u>치명적 망각 현상Catastrophic Forgetting 현상</u></strong>이 발생한다. 이를 해결하기 위해 (1)정규화 기법, (2)과거 데이터 활용, (3)파라미터 활용 등을 적용할 수 있다. 그러나, 최근 <strong><u>자기 주의집중Self-Attention</u></strong>을 활용한 <strong>트랜스포머Transformer</strong> 모델을 통해, 시공간의 개념을 분리하지 않아, 관찰한 사건들의 과거와 미래의 구분을 없애는 전략을 취한다. 저자의 말을 빌리자면, “과거를 예측하고, 미래를 회상한다”는 것이다!</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/7.png" style="width: 80%; height: auto;" />
    <figcaption>
      RNN &amp; LSTM &amp; GRU;The Neural Network Zoo[3]
    </figcaption>
  </figure>
</div>

<p>트랜스포머는 앞서 4장에서 다룬 기억의 구체화 문제에서도 효과적으로 작동한다. 시간 순서에 따른 사건들을 종합적으로 이해하는 추상화 과정과 달리, 구체화 과정에서는 순서에 맞게 사건을 재구성할 수 있어야 한다. 이 때 기억의 구체화 신경망은 주Q어진 상황에 맞게 재해석해서 사건을 순서대로 재구성하는 것을 목표로 한다. 이때 <strong>주의집중 마스킹 전략Attention Masking</strong>을 통해 현재까지의 입력만을 받도록 하여 BERT, GPT같은 좋은 성능을 달성할 수 있도록 하였다.</p>

<p>| Questions:</p>
<ol>
  <li>우리의 두뇌는 어떻게 Catastrophic Forgetting 문제를 해결하고 있는가?</li>
  <li>Transformer 모델이 정말 AE와 Attention 기능을 적절히 활용하고 있는가?
 (World Model의 경우, VAE+RNN+Controller로 구성되어 있다.)</li>
  <li>Transformer 모델의 계산 및 학습 효율성은 어떻게 되는가? 일반 GPU에서 적절히 사용 가능한가?</li>
  <li>Transformer 모델이 Neuroscience의 관점에서 효율적인가?</li>
</ol>

<hr />

<h1 id="6-생각이-시간과-공간을-넘나드는-마법을-부리다">6. 생각이 시간과 공간을 넘나드는 마법을 부리다</h1>

<p>챕터 1~3까지 개념의 추상화, 챕터 4에서는 개념의 구체화, 챕터 5에서는 기억에 대해 다루었다. 이제는 신경망의 학습 원리에 대해 다루고자 한다. 현재 신경망의 <strong>가중치 전달 과정Weight Transport</strong>에 사용되는 오차 역전파 알고리즘은 벨만의 동적 계획법을 활용한다. 일종의 <u>"전지적 작가 시점God Mode"</u>인 것이다. 정말 우리의 뇌도 이런 식으로 작동할까?</p>

<p>딥마인드의 연구 결과에 따르면, 심지어 가중치 정보를 제거한 후 난수를 학습시키는 <strong>역전파 정렬Feedback Alignment</strong>만으로도 학습이 가능했다! 적어도 생물학적 신경망은 전지적 작가 시점의 오차 역전파 알고리즘 대로 학습하지 않는 셈이다.</p>

<p>하나의 뉴런이 지니는 <strong>신경 가소성Synaptic Plasticity</strong>은 (1)장기 강화Long-Term Potentiation과 (2)장기 억압Long-Term Depression의 전체적인 변화 과정으로 구성된다. 이 두 과정은 뉴런들의 수상돌기가 지닌 동적 반응 특성(수상돌기의 민감도 변화)으로 일어나는 <strong>상대적인 타이밍 차이</strong>로 야기된다는 점을 고려하면, 생물학적 신경망은 전지적 작가 시점이 아닌 <u>"1인칭 주인공 시점First-Person Mode"</u>인 것이다.</p>

<p>연구자들은 신경 가소성을 인공 신경망에 담기 위해 시간적 오차 모델Temporal Error Model, 이후 예측 코딩 모델Predictive Coding Model을 개발했다. 예측 코딩 모델은 오류 노드를 포함하여 뉴런의 전달의 정확도를 평가한다.</p>

<p>한편, 수상돌기는 일종의 정규화 연산을 수행한다. 수상돌기의 크기가 큰 신경세포는 다양한 입력 채널의 입력값에 둔감하게 반응하게 되어 둔감성을 유지한다. 다시말해, 정보량의 차이가 커지더라도 안정적으로 작동한다는 것이다. 이때, 국소적인 연결이 많은 수상돌기는 <strong>기저수상돌기Basal Dendrite</strong>, 전역적인 연결이 많은 수상돌기는 <strong>선단수상돌기Apical Dendrite</strong>라고 부른다. 최근 연구에 따르면 기저수상돌기는 적어도 얕은 인공 신경망, 선단수상돌기는 하나 이상의 은닉층을 가진 신경망과 비슷한 연산 능력을 지니고 있다고 한다.</p>

<p>| Questions:</p>
<ol>
  <li>신경 가소성의 동적 특성을 바탕으로 한 학습 알고리즘이 있는가?</li>
  <li>물리적인 관점에서 흥미롭게 본 Critical Brain Hypothesis의 경우, 역전파 정렬 현상을 설명할 수 있을까?</li>
  <li>뉴런이 하나의 랜덤 변수라고 생각하면, 노이즈의 중요성은 무엇일까?</li>
  <li>예측 코딩보다 Active Inference가 더 좋은 설명을 제공해 줄 수 있을 것인가?</li>
</ol>

<hr />

<h1 id="7-미래를-내다보며-과거를-바꾼다">7. 미래를 내다보며 과거를 바꾼다</h1>
<p>챕터 1~3까지 개념의 추상화, 챕터 4에서는 개념의 구체화, 챕터 5에서는 기억, 챕터 6에서는 신경망의 학습 원리를 다루었다. 마지막 7장에서는 강화학습을 다루고자 한다. 강화학습은 기본적으로 동적 시스템의 전략 또는 정책을 학습하는 문제로 귀결된다. 다만, 전략이 실제로 수행될 때, 과거 전략이 현재의 경험에 얼마나 기여했는지에 대한 <strong><u>시간적 기여도 할당 문제Temporal Credit Assignment</u></strong>를 해결할 필요가 있다.</p>

<p>벨만의 동적 계획법Dynamic Programming과 최적성의 원리Principle of Optimality는 이러한 문제를 해결해주는 방법 중 하나로, 복잡한 문제를 쉬운 문제를 재귀적으로 풀어나가며 해결한다. 보다 구체적으로, Markov Decision Process라는 수학적 형식 위에서 벨만 방정석을 정의할 수 있고 주어진 환경으로부터 얻은 보상예측오류를 활용해 강화학습을 수행할 수 있다.</p>

<p>알파고와 같은 강화학습 알고리즘은 <strong>모델 프리 강화학습Model-Free Reinforcement Learning;MFRL</strong>을 활용하여 문제를 해결한다. 그러나 이러한 알파고의 습관적 행동을 분석해보면, 결과에 집착하는 행동Devaluation Insensitivity를 관찰할 수 있다. MFRL은 목적이 유연하게 변해야 하는 경우, 기민하게 행동하지 못한다는 단점을 갖고 있다. 실제로, 동물들의 행동을 관찰하면 대개 결과지향적 태도를 확인할 수 있다. 1977년 연구 결과(울프람 슐츠, 피터 다이안, 리드 몽타규)에 따르면, 원숭이의 중뇌 부위의 도파민 신경세포 활성도가 변화하는 모습이 모델 프리 학습 알고리즘의 보상 예측 오류 신호와 닮았다는 것을 밝힌다. 2004년에는 인간 중뇌의 선조체Striatum가 MFRL의 중추임을 발견했다!</p>

<p>이러한 단점을 해결할 수 있도록, 목표지향적 행동을 구현해내는 학습 방식을 <strong>모델 기반 강화학습Model-Based Reinforcement Learning;MBRL</strong>이 제안되어 왔다. 이 모델은 목표와 원리를 분리하여 다양한 상황의 다양한 목표를 달성할 수 있도록 설계되었다. 물론, 문제가 너무 복잡하거나 환경의 불확실성이 높아 모델을 학습하기 어려운 경우 잘 작동하지 않는다는 단점을 갖고 있다. MFRL과 유사하게, MBRL도 과학적 역사가 깊다. 1930년대 연구 결과(에드워드 톨만), 쥐 실험을 통해 보상이 없는 상황에서도 잠재학습Latent Learning이 발생하고 이를 통해 환경에 대한 정보인 <u>인지맵Cognitive Map</u>을 형성한다는 것을 밝힌다. 예상하는 상황과 현실 상황의 괴리로부터 상태 예측 오류를 통해 학습하는 것이다! 이후 쥐의 경우, 변역계Limbic System을 중심으로 변연계 아래피질Infralimbic Cortex, 인간의 경우, 측전전두피질Lateral Prefrontal Cortex와 두정엽내고랑Intraparietal Sulcus 등에서 MBRL의 중추임을 발견했다.</p>

<p>한편, 최적 제어 이론의 모델 예측 제어Model Predictive Control의 경우가 모델 기반의 방법론을 활용하며, 모델 기반 학습의 시초라고 평가되기도 한다. 행동심리경제학에서는 MFRL과 같이 반자동적이고 빠른 의사결정 방식을 “<strong>시스템 1</strong>”, MBRL과 같이 의식적이고 느린 의사결정 방식을 “<strong>시스템 2</strong>“라고 부른다.</p>

<p>2005년 런던 대학의 연구팀은 더욱 과감한 질문을 던지는데, 동물의 뇌가 결과를 얘측할 때의 불확실성에 따라 MFRL과 MBRL을 스위칭한다는 것이다. 실제로 2015년, 인간의 측전전두피질과 전두극피질Frontopolar Cortex에서 이러한 불확실성 정보를 처리하고 있고, 이를 바탕으로 두 전략을 조합하는 방식으로 학습 과정을 제어하고 있음을 밝혔다! 다시 말해, 주어진 ‘사건Event’에서 ‘상황Context’을 읽어내고, 그 속에서 ‘문제Task’에 초점을 맞추게 되는 메타 인지가 작동하는 셈이다.</p>

<p>| Questions:</p>
<ol>
  <li>MBRL과 MFRL의 장단점에 대해서 잘 찾아보자.</li>
  <li>두 패러다임과 대니얼 카너먼의 시스템 I vs II가 정말로 매칭되는 개념인가?</li>
  <li>메타 인지는 고차 사고를 의미하는가? MBRL과 MFRL의 스위칭 원리는 단순한 2차적 사고에 불과한가?</li>
  <li>Active Inference는 더욱 큰 설명력과 이론을 제공할 수 있겠는가?</li>
</ol>

<h3 id="reference">Reference</h3>

<p>[1] 인공지능과 뇌는 어떻게 생각하는가, 출판사 ‘솔’, 이상완 지음, 2022</p>

<p>[2] <a href="https://www.sciencetimes.co.kr/news/%EC%96%BC%EA%B5%B4-%EC%9D%B8%EC%8B%9D%ED%95%98%EB%8A%94-%EC%8B%A0%EA%B2%BD%EC%84%B8%ED%8F%AC-%EB%94%B0%EB%9D%BC-%EC%9E%88%EB%8B%A4/">얼굴 인식하는 신경세포 따로 있다, The Science Times, 심재율 객원기자, 2019 2/12, </a></p>

<p>[3] Van Veen, F., &amp; Leijnen, S. (2019). The Neural Network Zoo. The Asimov Institute. https://www.asimovinstitute.org/neural-network-zoo/</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#%EB%91%90%EB%87%8C" class="page__taxonomy-item p-category" rel="tag">두뇌</a><span class="sep">, </span>
    
      <a href="/tags/#%EC%8B%A0%EA%B2%BD%EA%B3%BC%ED%95%99" class="page__taxonomy-item p-category" rel="tag">신경과학</a><span class="sep">, </span>
    
      <a href="/tags/#%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5" class="page__taxonomy-item p-category" rel="tag">인공지능</a><span class="sep">, </span>
    
      <a href="/tags/#%EC%B1%85" class="page__taxonomy-item p-category" rel="tag">책</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#motivation" class="page__taxonomy-item p-category" rel="tag">Motivation</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2023-05-24T00:00:00+09:00">May 24, 2023</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5+vs+%EB%91%90%EB%87%8C%20http%3A%2F%2Flocalhost%3A4000%2Fmotivation%2FAI-vs-Brain-kr%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fmotivation%2FAI-vs-Brain-kr%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fmotivation%2FAI-vs-Brain-kr%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/motivation/AI-vs-Brain-en/" class="pagination--pager" title="AI vs. The Brain
">Previous</a>
    
    
      <a href="/motivation/About-Natualizing-Phenomenology-en/" class="pagination--pager" title="Implications and Limits of Naturalizing Phenomenology
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You May Also Enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/posts/2024/Q4/2024-10-23-Seven%20and%20a%20half%20lessons/sevenandahalf_lessons.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/motivation/Seven-and-a-half-lessons-kr/" rel="permalink">‘이토록 뜻밖의 뇌과학’을 읽고
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/posts/2024/Q4/2024-10-23-Seven%20and%20a%20half%20lessons/sevenandahalf_lessons.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/motivation/Seven-and-a-half-lessons-en/" rel="permalink">After reading ‘Seven and a half lessons’
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          9 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/500x300.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Belief-and-Mind-kr/" rel="permalink">Belief and mind/kr
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">믿음이란 무엇이고, 어떻게 가능한가?

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/500x300.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Probability-and-Meaning-kr/" rel="permalink">Probability and meaning/kr
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://x.com/HumMachCoevol" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/dohyeon-lee-4793a6244" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> Linkedin</a></li>
        
      
        
          <li><a href="https://www.youtube.com/@leadh99" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube</a></li>
        
      
        
          <li><a href="https://github.com/Lee-DoHyeon" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Do-Hyeon Lee. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
<script src="/assets/js/main.min.js"></script>
 

<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>  

    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/motivation/AI-vs-Brain-kr/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/motivation/AI vs Brain/kr"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://lee-dohyeon-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  
 

<script type="text/x-mathjax-config">

  MathJax.Hub.Config({

    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}

  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"
  async
></script>


  </body>
</html>
