<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="us" class="no-js">
  <head>
    <meta charset="utf-8" />

<!-- begin _includes/seo.html --><title>AI vs. The Brain  |  Do-Hyeon Lee</title>
<meta name="description" content="">


  <meta name="author" content="Do-Hyeon Lee">
  
  <meta property="article:author" content="Do-Hyeon Lee">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="us_EN">
<meta property="og:site_name" content="Do-Hyeon Lee">
<meta property="og:title" content="AI vs. The Brain">
<meta property="og:url" content="http://localhost:4000/motivation/AI-vs-Brain-en/">


  <meta property="og:description" content="">



  <meta property="og:image" content="http://localhost:4000/assets/images/posts/2023/Q2/2023-05-24-AI%20vs%20Brain/0.jpeg">





  <meta property="article:published_time" content="2023-05-24T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/motivation/AI-vs-Brain-en/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Do-Hyeon Lee",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->
 
<link
  href="/feed.xml"
  type="application/atom+xml"
  rel="alternate"
  title="Do-Hyeon Lee Feed"
/>


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0" />

<script>
  document.documentElement.className =
    document.documentElement.className.replace(/\bno-js\b/g, "") + " js ";
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css" />
<link
  rel="preload"
  href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"
  as="style"
  onload="this.onload=null;this.rel='stylesheet'"
/>
<noscript
  ><link
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"
/></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link
  rel="apple-touch-icon"
  sizes="152x152"
  href="/favicon.io/apple-touch-icon.png"
/>
<link
  rel="icon"
  type="image/png"
  sizes="32x32"
  href="/favicon.io/favicon-32x32.png"
/>
<link
  rel="icon"
  type="image/png"
  sizes="16x16"
  href="/favicon.io/favicon-16x16.png"
/>
<link rel="manifest" href="/favicon.io/site.webmanifest" />
<meta name="msapplication-TileColor" content="#da532c" />
<meta name="theme-color" content="#ffffff" />
<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Do-Hyeon Lee
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/research/">Research</a>
            </li><li class="masthead__menu-item">
              <a href="/watch/">Watch</a>
            </li><li class="masthead__menu-item">
              <a href="/read/">Read</a>
            </li><li class="masthead__menu-item">
              <a href="/outreach/">Outreach</a>
            </li><li class="masthead__menu-item">
              <a href="/cv/">CV</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/avatar.jpg" alt="Do-Hyeon Lee" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">Do-Hyeon Lee</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Human and Machine Mind-Behavior-Brain Researcher</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="https://maps.app.goo.gl/fDF8TMFHnu1fHCBd9" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i><span class="label">Location</span></a></li>
          
        
          
            <li><a href="mailto:lead.o.hyeon@gmail.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://x.com/HumMachCoevol" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i><span class="label">Twitter</span></a></li>
          
        
          
            <li><a href="https://www.linkedin.com/in/dohyeon-lee-4793a6244" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">Linkedin</span></a></li>
          
        
          
            <li><a href="https://www.youtube.com/@leadh99" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i><span class="label">YouTube</span></a></li>
          
        
          
            <li><a href="https://github.com/Lee-DoHyeon" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="AI vs. The Brain">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="2023-05-24T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/motivation/AI-vs-Brain-en/" class="u-url" itemprop="url">AI vs. The Brain
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          12 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> AI vs. The Brain</h4></header>
              <ul class="toc__menu"><li><a href="#1-capturing-an-infinite-world-in-finite-space">1. Capturing an Infinite World in Finite Space</a></li><li><a href="#2-todays-success-tomorrows-failure">2. Today’s Success, Tomorrow’s Failure</a></li><li><a href="#3-the-balance-between-sensitivity-and-insensitivity">3. The Balance Between Sensitivity and Insensitivity</a></li><li><a href="#4-the-subjective-is-so-objective">4. The Subjective is So Objective</a></li><li><a href="#5-predicting-the-past-and-remembering-the-future">5. Predicting the Past and Remembering the Future</a></li><li><a href="#6-thinking-beyond-time-and-space">6. Thinking Beyond Time and Space</a></li><li><a href="#7-changing-the-past-by-looking-into-the-future">7. Changing the Past by Looking into the Future</a><ul><li><a href="#references">References</a></li></ul></li></ul></li></ul>

            </nav>
          </aside>
        
        <style>
  .centered-container {
      text-align: center;
  }
  figure {
      display: inline-block;
      margin: auto;
      padding: 10px;
      text-align: center;
      background-color: #fff;
  }
  figcaption {
      font-family: "Wanted Sans Variable", "Wanted Sans";
      font-size: 12px;
      color: #555;
      margin-top: 5px;
  }
</style>

<blockquote>
  <p>“How does AI solve problems that seem too complex for us?
Why does AI struggle with tasks that are so simple for us?
What does human intelligence look like from the perspective of AI?”</p>
</blockquote>

<blockquote>
  <p>This post is a summary of Professor Sangwan Lee’s book “How AI and the Brain Think.”</p>
</blockquote>

<h1 id="1-capturing-an-infinite-world-in-finite-space">1. Capturing an Infinite World in Finite Space</h1>

<p>This chapter introduces the dilemma of abstraction and diversity, centered around the <strong>binding problem</strong>—how we recognize objects like an apple. It provides a historical overview of early AI research, starting with Marvin Minsky’s feedforward networks and the challenges of the Credit Assignment Problem, solved by the Error Backpropagation algorithm.</p>

<hr />

<h1 id="2-todays-success-tomorrows-failure">2. Today’s Success, Tomorrow’s Failure</h1>

<p>While the first chapter focused on current problems, the second chapter delves into statistical scenarios in real-world interactions. It introduces Structural Risk Minimization from the perspective of statistical machine learning, discussing Tikhonov Regularization to prevent underfitting/overfitting and introducing Support Vector Machines (SVM) through the Dual Problem.</p>

<p>The pursuit of “model efficiency” to solve the Bias-Variance Tradeoff is also found in the human brain. There’s even a neuron that recognizes the actress Jennifer Aniston [2]! This suggests the concept of <strong>sparse representation</strong>, where very few active cells are required to stably recognize a concept. This ties into critical periods in developmental cognition and pruning, which are key to energy efficiency—an area where the human brain is at least 400 times more efficient than AI systems.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/1.jpeg" style="width: 80%; height: auto;" />
    <figcaption>
      The biggest difference between the brain and AI systems: energy efficiency!
    </figcaption>
  </figure>
</div>

<hr />

<h1 id="3-the-balance-between-sensitivity-and-insensitivity">3. The Balance Between Sensitivity and Insensitivity</h1>

<p>Having addressed the basics of abstraction/diversity in the first chapter and the statistical solutions and brain characteristics in the second, the third chapter tackles the Specificity-Invariance Dilemma. To achieve both sensitivity and insensitivity in concept abstraction, three approaches are discussed: (1) filtering, (2) pooling, and (3) locality.</p>

<p>First, (1) filtering uses convolution operations to filter out specific patterns, allowing us to focus on important information while ignoring the rest. Next, (2) pooling or subsampling compresses the information, extracting only what is meaningful. Finally, (3) locality applies to both filtering and pooling, setting the receptive field’s range to focus on significant local information. Such <strong>topographic maps</strong> also appear in the brain, implying that inductive biases are intentionally applied to enhance the <u>spatial, temporal, and energy efficiency</u> of computation. While the origins of this are still unclear, it is suggested that these traits may have been acquired through evolution.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/2.jpeg" style="width: 80%; height: auto;" />
    <figcaption>
      Topographic patterns in the motor cortex. Could these local arrangements enhance computational efficiency?
    </figcaption>
  </figure>
</div>

<p>By combining convolution, pooling, and non-linear functions into building blocks and stacking them in various ways, architectures can be designed to achieve the desired level of abstraction. While this bottom-up approach looks at the whole through its parts, the top-down approach allows us to focus on the parts through the whole, enabled by attention mechanisms. Although Self-Attention has emerged, a clean solution is yet to be found.</p>

<p>Our brain resolves the paradox between parts and the whole through bottom-up concept abstraction and top-down attention. A mechanism exists that can capture high-level concepts even from the lowest layers. This is possible thanks to the nonlinearity of retinal neurons, where the density decreases from the center to the periphery, guiding top-level attention by leveraging low-dimensional peripheral information.</p>

<ul>
  <li>Brain -&gt; AI:
    <ul>
      <li>It took 20 years for CNNs, reflecting the computational principles of the cerebral cortex, to achieve great success.</li>
      <li>Brain science could have accelerated this process.</li>
    </ul>
  </li>
  <li>AI -&gt; Brain:
    <ul>
      <li>The functional similarity between CNNs and brain function highlights how the brain processes information at each layer, showing a mutually beneficial relationship.</li>
      <li>Predictive Coding Scheme, Illusion?</li>
    </ul>
  </li>
</ul>

<p>| Questions:</p>
<ol>
  <li>If artificial neural networks are functionally similar to the brain, could we observe similar phenomena from an introspective perspective as well? How do human optical illusions relate to phenomena in artificial neural networks?</li>
  <li>Does the local arrangement of representations in the cerebral cortex genuinely enhance the spatial, temporal, and energy efficiency of computation? Could this be verified through computational models and simulations?</li>
</ol>

<hr />

<h1 id="4-the-subjective-is-so-objective">4. The Subjective is So Objective</h1>

<p>Chapters 1-3 focused on how we abstract and understand the world based on observed results. Chapter 4 shifts to the opposite direction—concretization, with a focus on data generation. Unlike the deterministic neural networks discussed earlier, we now explore <strong>probabilistic neural networks</strong> where each neuron is replaced by a random variable.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/3.png" style="width: 80%; height: auto;" />
    <figcaption>
      Hopfield Network, Boltzmann Machine, Restricted BM, The Neural Network Zoo [3]
    </figcaption>
  </figure>
</div>

<p>An early example of probabilistic neural networks is the <strong><u>Hopfield Network</u></strong>, which allows not only simple statistical analysis but also complex correlation analysis. Probabilistic neural networks are advantageous because they (1) allow easy inference, (2) explain the roles of each element, (3) highlight the risks of ignoring certain elements, and (4) detect intricate interrelationships.</p>

<p>The <strong><u>Boltzmann Machine</u></strong>, which adds hidden neurons to the Hopfield Network, allows hypotheses to be tested based on data. Its goal is to learn the correlations between various factors in the data. It does this by deriving learning rules to find the optimal weights that connect neurons, minimizing the difference between (1) observable correlations in the data and (2) the correlations learned by the network. The former is akin to being in a wakeful state, directly observing data, while the latter is like a dream state, where the network infers without direct data observation—hence, the Wake-Sleep Rule.</p>

<p>To address the difficulty of applying probabilistic neural networks in practice, the <strong><u>Restricted Boltzmann Machine (RBM)</u></strong> was developed. RBM allows connections only between visible and hidden neurons, eliminating complex correlations among hidden neurons. The structure of visible and hidden neuron layers forms a building block, leading to the development of Deep RBM (DRBM). When combined with the error backpropagation algorithm for precise learning, it becomes the deterministic Autoencoder (AE).</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/4.png" style="width: 80%; height: auto;" />
    <figcaption>
      Autoencoder &amp; Variational Autoencoder, The Neural Network Zoo [3]
    </figcaption>
  </figure>
</div>

<p>Autoencoders use symmetry to achieve abstraction and concretization, a principle also seen in Principal Component Analysis (PCA). Today, even if symmetry is broken, it is utilized within mathematically permissible limits. The probabilistic version of the autoencoder, the <strong><u>Variational Autoencoder (VAE)</u></strong>, (1) retains some symmetry of the autoencoder, (2) adds depth to the abstraction process, and (3) increases the density of the concretization process. Lastly, the <strong><u>Reparametrization</u></strong> trick is applied, allowing the error backpropagation algorithm to be used.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/5.png" style="width: 80%; height: auto;" />
    <figcaption>
      GAN, The Neural Network Zoo [3]
    </figcaption>
  </figure>
</div>

<p>Ultimately, the performance of abstraction and concretization is evaluated by the error function defined by the user. To objectify this self-assessment, game theory is employed, leading to models like the <strong><u>Generative Adversarial Network (GAN)</u></strong>. From an information theory perspective, traditional deterministic networks like PCA and autoencoders learn by minimizing KL Divergence. Unlike the asymmetric KL Divergence, GAN minimizes the symmetric JS Divergence, which can be viewed as an Optimal Transport problem in economics and evaluated using the Wasserstein Distance.</p>

<p>| Questions:</p>
<ol>
  <li>How do these probabilistic neural networks relate to the human brain? Are neurons treated as random variables because of their inherent randomness, or is this merely an assumption?</li>
  <li>How do recent diffusion-based generative models like StableDiffusion relate to the models discussed above?</li>
  <li>How are they connected to the Ising Model in statistical mechanics or Cellular Automata?</li>
</ol>

<hr />

<h1 id="5-predicting-the-past-and-remembering-the-future">5. Predicting the Past and Remembering the Future</h1>

<p>Chapters 1-3 covered concept abstraction, Chapter 4 dealt with concept concretization, and now, Chapter 5 focuses on memory—taking concepts on a journey through time. Previously, the i.i.d condition for data was assumed, but in reality, most events are not independent. Therefore, remembering concepts generated in the past becomes crucial. A strategy to implement this is <strong>feedback</strong>. One of the most common neurons found in the cerebral cortex, hippocampus, and other areas is the <strong><u>Pyramidal Neuron</u></strong>, which receives input not only from nearby neurons but also from distant neurons it has already communicated with! In <strong><u>Recurrent Neural Networks (RNN)</u></strong>, applying the backpropagation algorithm can lead to problems like the <u>exploding gradient</u> or <u>vanishing gradient</u>, making learning difficult. To address these issues, <strong><u>Long-Short Term Memory (LSTM)</u></strong> was developed.</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/6.png" style="width: 80%; height: auto;" />
    <figcaption>
      RNN, LSTM, and GRU; The Neural Network Zoo [3]
    </figcaption>
  </figure>
</div>

<p>The ideal neural network that can flexibly accept new information while maintaining stable memory would (1) adapt to situations and (2) expand its knowledge. This is known as <strong>Continual Learning</strong> or <strong>Lifelong Learning</strong>. Moreover, this adaptability makes it useful for Multi-Domain Adaptation and Multi-Task Learning.</p>

<p>Artificial neural networks lack this fluid memory capability, leading to a phenomenon called <strong><u>Catastrophic Forgetting</u></strong>, where past learning evaporates in pursuit of new information. To mitigate this, techniques like (1) regularization, (2) leveraging past data, and (3) parameter utilization can be applied. However, the Transformer model, which uses <strong><u>Self-Attention</u></strong>, eliminates the distinction between past and future events in space and time. As the author puts it, “predict the past and remember the future!”</p>

<div class="centered-container">
  <figure>
    <img src="/assets/images/posts/2023/Q2/2023-05-24-AI vs Brain/7.png" style="width: 80%; height: auto;" />
    <figcaption>
      RNN, LSTM, and GRU; The Neural Network Zoo [3]
    </figcaption>
  </figure>
</div>

<p>The Transformer model is also effective in addressing the problem of concretizing memory discussed in Chapter 4. While abstraction involves comprehensively understanding events in sequence, concretization requires reconstructing events in the correct order. The memory concretization network aims to reinterpret and reorder events according to the given context. Techniques like <strong>Attention Masking</strong> allow the model to focus only on the inputs received so far, enabling high performance in models like BERT and GPT.</p>

<p>| Questions:</p>
<ol>
  <li>How does the human brain solve the problem of Catastrophic Forgetting?</li>
  <li>Does the Transformer model truly leverage the AE and Attention mechanisms effectively?
 (In the case of the World Model, it consists of VAE+RNN+Controller.)</li>
  <li>What are the computational and training efficiencies of the Transformer model? Can it be used effectively on a standard GPU?</li>
  <li>Is the Transformer model efficient from a neuroscience perspective?</li>
</ol>

<hr />

<h1 id="6-thinking-beyond-time-and-space">6. Thinking Beyond Time and Space</h1>

<p>Chapters 1-3 focused on concept abstraction, Chapter 4 on concept concretization, Chapter 5 on memory, and now, Chapter 6 deals with the learning principles of neural networks. The error backpropagation algorithm, used in the <strong>weight transport process</strong> of current neural networks, relies on Bellman’s Dynamic Programming—a sort of <u>"God Mode."</u> But does our brain really operate this way?</p>

<p>Research from DeepMind suggests that even removing weight information and training with random numbers using only <strong>Feedback Alignment</strong> can enable learning! At least biological neural networks do not learn according to a God Mode error backpropagation algorithm.</p>

<p>The <strong>Synaptic Plasticity</strong> of a neuron consists of the overall changes through (1) Long-Term Potentiation and (2) Long-Term Depression. These changes are caused by the <strong>relative timing differences</strong> that result from the dynamic response characteristics of the neuron’s dendrites (changes in dendritic sensitivity), indicating that biological neural networks operate more like a <u>"First-Person Mode"</u> rather than a God Mode.</p>

<p>To capture synaptic plasticity in artificial neural networks, researchers developed the Temporal Error Model and later the Predictive Coding Model. The Predictive Coding Model includes error nodes to assess the accuracy of neuron transmission.</p>

<p>On the other hand, dendrites perform a kind of normalization operation. Large dendrites respond insensitively to the input values of various channels, maintaining insensitivity and stability even when the information volume changes significantly. Dendrites with many local connections are called <strong>Basal Dendrites</strong>, while those with many global connections are <strong>Apical Dendrites</strong>. Recent research suggests that Basal Dendrites are comparable to shallow artificial neural networks, while Apical Dendrites are akin to deep neural networks with one or more hidden layers.</p>

<p>| Questions:</p>
<ol>
  <li>Are there learning algorithms based on the dynamic characteristics of synaptic plasticity?</li>
  <li>Can the Critical Brain Hypothesis, which I find intriguing from a physical perspective, explain the phenomenon of Feedback Alignment?</li>
  <li>If we consider a neuron as a random variable, what is the significance of noise?</li>
  <li>Could Active Inference provide an even better explanation than Predictive Coding?</li>
</ol>

<hr />

<h1 id="7-changing-the-past-by-looking-into-the-future">7. Changing the Past by Looking into the Future</h1>

<p>Chapters 1-3 covered concept abstraction, Chapter 4 dealt with concept concretization, Chapter 5 focused on memory, and Chapter 6 discussed the learning principles of neural networks. The final chapter, Chapter 7, explores reinforcement learning. At its core, reinforcement learning is about learning strategies or policies for dynamic systems. However, when these strategies are applied, the <strong>Temporal Credit Assignment Problem</strong>—determining how much past strategies contributed to the current experience—must be addressed.</p>

<p>Bellman’s Dynamic Programming and the Principle of Optimality are some ways to solve this problem by recursively breaking down complex problems into simpler ones. More specifically, Bellman’s equations are defined over a Markov Decision Process, and reinforcement learning is performed using the reward prediction error derived from the environment.</p>

<p>Reinforcement learning algorithms like AlphaGo solve problems using <strong>Model-Free Reinforcement Learning (MFRL)</strong>. However, analyzing AlphaGo’s habitual behavior reveals Devaluation Insensitivity—a fixation on results. MFRL struggles with flexibility when goals need to change. In contrast, animals typically display goal-oriented behavior. Research from 1977 (Wolfram Schultz, Peter Dayan, Read Montague) revealed that changes in dopamine neuron activity in monkeys resemble the reward prediction error signals in Model-Free learning algorithms. In 2004, the human striatum was identified as the central region for MFRL!</p>

<p>To overcome these shortcomings, <strong>Model-Based Reinforcement Learning (MBRL)</strong> was proposed, which separates goals from principles, allowing for diverse goals in various situations. However, this model struggles when problems are too complex or environmental uncertainty is high, making it difficult to learn the model. Like MFRL, MBRL has a deep scientific history. Research from the 1930s (Edward Tolman) showed that rats could engage in latent learning even without rewards, forming a <u>Cognitive Map</u> of the environment. This learning occurs through state prediction errors arising from discrepancies between expected and actual situations. It was later discovered that the Limbic System in rats and the Lateral Prefrontal Cortex and Intraparietal Sulcus in humans are central to MBRL.</p>

<p>Meanwhile, Model Predictive Control in optimal control theory, which uses model-based methodologies, is considered the origin of model-based learning. In behavioral economics, the quick, semi-automatic decision-making style like MFRL is known as “<strong>System 1</strong>,” while the conscious, slow decision-making style like MBRL is referred to as “<strong>System 2</strong>.”</p>

<p>In 2005, a research team at University College London posed an even bolder question: could the brain switch between MFRL and MBRL depending on the uncertainty in predicting outcomes? Indeed, in 2015, it was revealed that the Lateral Prefrontal Cortex and Frontopolar Cortex in humans process this uncertainty information and control the learning process by combining the two strategies! In other words, this meta-cognitive process allows us to “read the situation,” focus on the “task,” and adapt our learning strategies accordingly.</p>

<p>| Questions:</p>
<ol>
  <li>Let’s explore the advantages and disadvantages of MBRL and MFRL.</li>
  <li>Are these two paradigms truly analogous to Daniel Kahneman’s System I vs. II?</li>
  <li>Does meta-cognition imply higher-order thinking? Is the switching principle between MBRL and MFRL merely secondary thinking?</li>
  <li>Can Active Inference offer a more comprehensive theory?</li>
</ol>

<h3 id="references">References</h3>

<p>[1] “How AI and the Brain Think,” by Sangwan Lee, Sol Publishing, 2022</p>

<p>[2] <a href="https://www.sciencetimes.co.kr/news/%EC%96%BC%EA%B5%B4-%EC%9D%B8%EC%8B%9D%ED%95%98%EB%8A%94-%EC%8B%A0%EA%B2%BD%EC%84%B8%ED%8F%AC-%EB%94%B0%EB%9D%BC-%EC%9E%88%EB%8B%A4/">There Are Neurons that Recognize Faces, The Science Times, Sim Jaeyul, Guest Reporter, 2019 2/12</a></p>

<p>[3] Van Veen, F., &amp; Leijnen, S. (2019). The Neural Network Zoo. The Asimov Institute. https://www.asimovinstitute.org/neural-network-zoo/</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#ai" class="page__taxonomy-item p-category" rel="tag">AI</a><span class="sep">, </span>
    
      <a href="/tags/#book" class="page__taxonomy-item p-category" rel="tag">Book</a><span class="sep">, </span>
    
      <a href="/tags/#brain" class="page__taxonomy-item p-category" rel="tag">Brain</a><span class="sep">, </span>
    
      <a href="/tags/#neuroscience" class="page__taxonomy-item p-category" rel="tag">Neuroscience</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#motivation" class="page__taxonomy-item p-category" rel="tag">Motivation</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2023-05-24T00:00:00+09:00">May 24, 2023</time></p>

      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=AI+vs.+The+Brain%20http%3A%2F%2Flocalhost%3A4000%2Fmotivation%2FAI-vs-Brain-en%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fmotivation%2FAI-vs-Brain-en%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fmotivation%2FAI-vs-Brain-en%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/motivation/AIF-and-Consciousness-kr/" class="pagination--pager" title="의식에 대한 능동추론적 관점
">Previous</a>
    
    
      <a href="/motivation/AI-vs-Brain-kr/" class="pagination--pager" title="인공지능 vs 두뇌
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You May Also Enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/posts/2024/Q4/2024-10-23-Seven%20and%20a%20half%20lessons/sevenandahalf_lessons.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/motivation/Seven-and-a-half-lessons-kr/" rel="permalink">‘이토록 뜻밖의 뇌과학’을 읽고
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/500x300.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Belief-and-Mind-kr/" rel="permalink">Belief and mind/kr
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          less than 1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">믿음이란 무엇이고, 어떻게 가능한가?

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/500x300.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/Probability-and-Meaning-kr/" rel="permalink">Probability and meaning/kr
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          1 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <img src="/assets/images/posts/2024/Q3/2024-09-24-Energy%20Based%20Net%20for%20Top%20Opt/TopOpt%20of%20a%20Compliance%20Prob.png" alt="">
      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/meditation/Energy-Based-Net-for-Top-Opt-kr/" rel="permalink">에너지 기반 네트워크를 활용한 형상 최적화
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://x.com/HumMachCoevol" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://www.linkedin.com/in/dohyeon-lee-4793a6244" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> Linkedin</a></li>
        
      
        
          <li><a href="https://www.youtube.com/@leadh99" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-youtube" aria-hidden="true"></i> YouTube</a></li>
        
      
        
          <li><a href="https://github.com/Lee-DoHyeon" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2024 Do-Hyeon Lee. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
<script src="/assets/js/main.min.js"></script>
 

<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>  

    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/motivation/AI-vs-Brain-en/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/motivation/AI vs Brain/en"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://lee-dohyeon-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  
 

<script type="text/x-mathjax-config">

  MathJax.Hub.Config({

    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}

  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"
  async
></script>


  </body>
</html>
